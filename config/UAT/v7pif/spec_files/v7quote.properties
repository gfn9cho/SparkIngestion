########################################################################################
	
		Property File to be used for the EDF Process.
	
#######################################################################################

# processName is used to capture the load statistics in the audit table.
# Partitioning is based on the this name in the audit table.
spark.DataIngestion.processName:v7quotemthlyDataExtract


# LoadType defines the frequency of load and type.
# TL-Truncate and Load, DI,MI,WI- Daily, Monthly & Weekly incremental loads respctivley, CI- Continous Ingestion Load
spark.DataIngestion.loadType:DI
#spark.DataIngestion.loadType:TL


# Restartabilty Indicator is used to reload the table that are failed to write to S3 or a count mismatch event.
# When set to Y - EDF will fetch all the failed table for the day and reload the respective partitions.
spark.DataIngestion.restartabilityInd:N


# Restartabilty level along with the restability Indicator, will specify the restarabilty level between set of tables or entire batch.
# When set to table - EDF will reload the failed table from the previous batches for the day.
# When set to batch - EDF will skip the table that are successfully loaded in the last batch and load the rest of the tables in a batch.
spark.DataIngestion.restartabilityLevel:table


# batchParallelism defines the degree of asynchronus data load.
# This number defines how many tables are read from database and written to S3 in parallel.
spark.DataIngestion.batchParallelism:5

# ConsiderBatchWindow will be used to switch between the batch window timings vs using max(updatetime) in the hive target table.
spark.DataIngestion.considerBatchWindow:N

# batchEndCutOff along with the loadType - CI, will define the cutt off time for the EDF on a day.
# To end the continous load at 7:00 PM EST, specify 18-59
spark.DataIngestion.batchEndCutOff:19-59


# repl_table specifies how EDF will get the batch window timing  between database replication table vs database sysdate.
# When blank, database system date will be used, otherwise the given repl_table will be queried for the batch window.
spark.DataIngestion.repl_table:


# timeLagInMins along with the batchWindowTiming will determine the batch window end time.
# To limit the batch window timing to current system time minus 2 mins, specify -2.
#spark.DataIngestion.timeLagInMins:10
spark.DataIngestion.timeLagInMins:0


# lookUpFile specifies the typelist joins to be performed before inserting into harmonized layer in S3.
# If no lookup required, provide an empty file with only the header.
spark.DataIngestion.lookUpFile:v7quote_LookupAnalysis.csv


# tableFile specifies the list of tables to be ingested.
spark.DataIngestion.tableFile:v7quote_TableSpec.csv


# piiFile specifies the list of pii columns that needs to be masked.
# specify an empty file with only the header, if no PII found.
spark.DataIngestion.piiFile:v7quote_PIISpec.csv


# DB Info & Credentials
# Provide an encrypted password which will be decrypted inside EDF.
spark.DataIngestion.dbHost:SADC1SQLP11
#spark.DataIngestion.dbHost:SACOLSQLT11
spark.DataIngestion.dbUser:edf_lakeRO
spark.DataIngestion.dbPwd:Iwn[j^tzwYwzhp(6:
spark.DataIngestion.sourceDB:V7M_PIF_QUOTE_DB
spark.DataIngestion.dbType:SqlServer

# TL DB Info & Credentials
spark.DataIngestion.dbTLType:SqlServer
spark.DataIngestion.dbTLHost:SADC1SQLP11
spark.DataIngestion.dbTLUser:edf_lakeRO
spark.DataIngestion.dbTLPwd:Iwn[j^tzwYwzhp(6:
spark.DataIngestion.sourceTLDB:SABIR_METAACT


# targetDB will be used to prefix the hive database name.
# connect_gwpc specifies the database prefix for policy center which is transformed as connect_gwpc_data_processed in reference to the Glue catalog.
spark.DataIngestion.targetDB:uat_lgcy_v7pif_data_processed
#spark.DataIngestion.targetDB:uat_lgcy_v7pif
#spark.DataIngestion.targetDB:uat_processed_v7pif


# auditDB will be used to prefix the hive database name.
spark.DataIngestion.auditDB:uat_dataingestion
#spark.DataIngestion.auditDB:uat_dataingestion_temp


# secured DB prefix will be used to load the secured tables data, which transforms simlilar to the target DB.
# connect_secured_gwpc to LGCY_v7PIF_secured_data_processed
spark.DataIngestion.targetSecuredDB:uat_lgcy_v7pif_secured_data_processed
#spark.DataIngestion.targetSecuredDB:uat_lgcy_v7pif_secured
#spark.DataIngestion.targetSecuredDB:uat_processed_secured_v7pif


# s3 paths for the harmonized and secured data.
spark.DataIngestion.hrmnzds3SecurePath:s3://sa-l3-uat-emr-edl-processed-secure/lgcyv7pif
spark.DataIngestion.hrmnzds3Path:s3://sa-l3-uat-emr-edl-processed/lgcyv7pif
spark.DataIngestion.auditPath:s3://sa-l3-uat-emr-edl-processed/dataIngestion
