########################################################################################
	
		Property File to be used for the EDF Process.
   
#######################################################################################

# processName is used to capture the load statistics in the audit table.
# Partitioning is based on the this name in the audit table.
spark.DataIngestion.processName:gwccDataStagingIncr

# Set this parameter to true when a schema check is desired.  Normally, this should be set to false.
spark.DataIngestion.schemaCheck:false

# LoadType defines the frequency of load and type.
# TL-Truncate and Load, DI,MI,WI- Daily, Monthly & Weekly incremental loads respctivley, CI- Continous Ingestion Load
spark.DataIngestion.loadType:DI


# Restartabilty Indicator is used to reload the table that are failed to write to S3 or a count mismatch event.
# When set to Y - EDF will fetch all the failed table for the day and reload the respective partitions.
spark.DataIngestion.restartabilityInd:N


# Restartabilty level along with the restability Indicator, will specify the restarabilty level between set of tables or entire batch.
# When set to table - EDF will reload the failed table from the previous batches for the day.
# When set to batch - EDF will skip the table that are successfully loaded in the last batch and load the rest of the tables in a batch.
spark.DataIngestion.restartabilityLevel:table


# restartTableIdentifier sets the loadStatus that needs to be considered for the restart Mode.
# by Default, this property takes failed status from audit table. For any custom status, modify the below parameter accordingly for reloading the previously failed JObs.
# This setting affects only the table level restartability.
spark.DataIngestion.restartTableIdentifier:failedUnknown


# DeleteBatch Indicator when set to Y will run the ingestion process for hard deletes.
spark.DataIngestion.hardDeleteBatch:N


# partitionOverwriteMode defined the target partition mode of whether to Overwrite or Append a partition.
# By Default, this property takes static.
# For incremental load that involved staging layer, this needs to be dynamic
spark.DataIngestion.partitionOverwriteMode:static


# batchParallelism defines the degree of asynchronus data load.
# This number defines how many tables are read from database and written to S3 in parallel. --was 50 for TL, 25 for DI/CI
spark.DataIngestion.batchParallelism:10

# ConsiderBatchWindow will be used to switch between the batch window timings vs using max(updatetime) in the hive target table.
spark.DataIngestion.considerBatchWindow:N

# batchEndCutOff along with the loadType - CI, will define the cutt off time for the EDF on a day.
# To end the continous load at 6:00 PM EST, specify 17-59
spark.DataIngestion.batchEndCutOff:17-59

# This parameter is used to define the iteration for harDeleteBatch.
# This when set on CI mode, will start the hardDeleteBatch at this Iteration.
spark.DataIngestion.hardDeleteBatchIter:3


# email Address to be captured to send the notification email
spark.DataIngestion.fromEmail:edfdatalake@stateauto.com
spark.DataIngestion.toEmail:edlakebuild@stateauto.com,edlakerun@stateauto.com
spark.DataIngestion.ccEmail:


# repl_table specifies how EDF will get the batch window timing  between database replication table vs database sysdate.
# When blank, database system date will be used, otherwise the given repl_table will be queried for the batch window.
spark.DataIngestion.repl_table:


# timeLagInMins along with the batchWindowTiming will determine the batch window end time.
# To limit the batch window timing to current system time minus 2 mins, specify 2.
# For staging incremental testing, we initially set this to 40 days prior, which is 57600 minutes
spark.DataIngestion.timeLagInMins:0


# lookUpFile specifies the typelist joins to be performed before inserting into harmonized layer in S3.
# If no lookup required, provide an empty file with only the header.
spark.DataIngestion.lookUpFile:gwcc_lookup_info.csv


# tableFile specifies the list of tables to be ingested.
spark.DataIngestion.tableFile:gwcc_table_spec.csv


# piiFile specifies the list of pii columns that needs to be masked.
# specify an empty file with only the header, if no PII found.
spark.DataIngestion.piiFile:gwcc_pii_spec.csv

# specifies the list of tables to be loaded into staging
spark.ingestion.stageTableList:gwcc_staging_list.csv

# DB Info & Credentials
# Provide an encrypted password which will be decrypted inside EDF.
spark.DataIngestion.dbHost:SAE1EGWSQLS23
spark.DataIngestion.dbUser:edf_lakeRO
spark.DataIngestion.dbPwd:Iwn[j^tzwYwzhp(6:
spark.DataIngestion.sourceDB:ClaimCenter


# stagePartitionBy will be used to specify the fieldName from the sourceDb which is used to partition staging data load
spark.DataIngestion.stagePartitionBy:updatetime

# stageTablePrefix will be used to specify the prefix for the staging tables.
spark.DataIngestion.stageTablePrefix:stg_gwcc_


# targetDB will be used to prefix the hive database name.
# connect_gwcl specifies the database prefix for PCLITE which is transformed as connect_gwcl_data_processed in reference to the Glue catalog.
spark.DataIngestion.targetDB:uatrun_processed_gwcc
spark.DataIngestion.targetStageDB:uatrun_staging_gwcc
spark.ingestion.reconResultDB:uatrun_audit_ingestion

# When the stageCutOffLimit is set to MIDNIGHT, data will be loaded until previous midnight.
# If the stageCutOffTime is provided, data will be loaded until then.
# Otherwise data will be loaded until current replication time.
spark.ingestion.stageCutOffLimit:None
spark.ingestion.stageCutOffTime:

# auditDB will be used to prefix the hive database name.
spark.DataIngestion.auditDB:uatrun_audit_ingestion

# the below parameter determines, if the source is connect source or not
# have to mark this as false when in ConnectV9-CL so that the process uses the coverage lookup tables in the join 
#     (becasue when this is true, all lookups have to be typelist tables)
spark.ingestion.isconnectdatabase:true

# specifies where this is a staging batch or an ingestion batch.
# For staging load, set this parameter to true 
spark.ingestion.stageLoadBatch:true
spark.ingestion.s3SyncEnabled:true
spark.ingestion.loadOnlyTLBatch:true
spark.ingestion.loadFromStage:false

#Determines whether the typelist has to be refreshed from source or the cono=solidatedTypelist can be used.
spark.ingestion.isTypeListToBeRefreshed:false

#create the empty tables even source data is not available
spark.ingestion.emptytableLoadFlag:true
validationBeforeHDRequired:false
spark.ingestion.extractProcessName:gwccDataExtract

# secured DB prefix will be used to load the secured tables data, which transforms simlilar to the target DB.
# connect_secured_gwcl to connect_secured_gwcl_data_processed
spark.DataIngestion.targetSecuredStageDB:uatrun_staging_secured_gwcc
spark.DataIngestion.targetSecuredDB:uatrun_processed_secured_gwcc

# s3 paths for the harmonized and secured data.
spark.DataIngestion.hrmnzds3SecurePath:s3://sa-l3-uat-datalake-processed-secure/uatrun/processed_secured_gwcc
spark.DataIngestion.stageS3SecurePath:s3://sa-l3-uat-datalake-staging-secure/uatrun/staging_secured_gwcc
spark.DataIngestion.hrmnzds3Path:s3://sa-l3-uat-datalake-processed/uatrun/processed_gwcc
spark.DataIngestion.stageS3Path:s3://sa-l3-uat-datalake-staging/uatrun/staging_gwcc
spark.ingestion.reconResultPath:s3://sa-l3-uat-datalake-processed/uatrun/audit_ingestion
spark.DataIngestion.auditPath:s3://sa-l3-uat-datalake-processed/uatrun/audit_ingestion